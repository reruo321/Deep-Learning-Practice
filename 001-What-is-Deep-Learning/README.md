# Unit 1 - What is Deep Learning?
## Mechanism
![001weight](https://user-images.githubusercontent.com/48712088/202853688-a912b524-b96a-4b06-b53f-af35fcdcc51c.jpg)

## History of Machine Learning
### Probabilistic Modeling

#### 1. Naive Bayes Algorithm
Easy Guide: https://www.youtube.com/watch?v=O2L2Uv9pdDA

**Naive Bayes classifier** is one of simple probabilistic classifiers based on applying Bayes' theorem with *strong (naive) independence assumptions between the features*. Bias ↑, Variance ↓.

* **Bayes' Theorem**: The theorem describing the probability of an event, based on prior knowledge of conditions that might be related to the event.
* **Prior Probability**: The probability distribution that would express one's belief, in Bayesian statistical inference.
* **Likelihood**: The probability of something discrete and individual. (excluding something continuous like weight or height)
* **Bias**: The inability for a machine learning method to capture the true relationship.
* **Variance**: The amount by which the prediction would change, if we fit the model to a different training data set.
* **Consequence of Variance**: The difference in fits between data sets.
* **Overfitting**: The production of an analysis too fitting to a particular set of data → may fail to fit to additional data or predict future observations reliably.
* **Underfitting**: It occurs when a mathematical model cannot adequately capture the underlying structure of the data.
* **Regularization**: A process that changes the result answer to be "simpler": To obtain results for ill-posed problems OR to prevent overfitting.
* **Ensemble Learning**: Use multiple learning algorithms → Obtain better predictive performance than just using one algorithm. Regularization, boosting, and bagging.

#### 2. Logistic Regression

#### 3. Multilayer Perceptrons
When these ANNs are trained under an appropriate loss function, they are naturally probabilistic.
